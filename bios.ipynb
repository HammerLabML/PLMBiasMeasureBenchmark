{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704a4ef-003a-48f0-af70-d63e34cb54b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, RocCurveDisplay, ConfusionMatrixDisplay\n",
    "\n",
    "from embedding import BertHuggingface\n",
    "from geometrical_bias import SAME, WEAT, GeneralizedWEAT, DirectBias, MAC, normalize, cossim, EmbSetList, EmbSet, GeometricBias\n",
    "from utils import CLFHead, SimpleCLFHead, CustomModel, JigsawDataset, BiosDataset, DebiasPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50bbf3d-7afa-4d55-8c52-c1ca51d2271d",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- check if debias reduces extrinsic biases\n",
    "- plot correlation (job-wise vs. overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246ead7-9c95-450f-adcc-b4501da08a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/bios_20421/config.json', 'r') as f:\n",
    "    exp_config = json.load(f)\n",
    "\n",
    "save_file = exp_config['save_dir']+'res.pickle' #results/bios1/res.pickle' #res_bios.pickle\n",
    "with open(save_file, 'rb') as handle:\n",
    "    res = pickle.load(handle)\n",
    "    exp_parameters = res['params']\n",
    "    results = res['results']\n",
    "    #results_test = res['results_eval']\n",
    "\n",
    "cosine_scores = {'SAME': SAME, 'WEAT': WEAT, 'gWEAT': GeneralizedWEAT, 'DirectBias': DirectBias, 'MAC': MAC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62965627-bc50-4e77-8d89-a73b1e49de9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd742c-0c05-4b98-809e-246356440c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(exp_parameters) == len(results), \"shape mismatch: \"+str(len(exp_parameters))+\" vs. \"+str(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae18f2-f2ad-457d-89ac-102ffc3e8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d5553-3f5f-49ca-9490-b726d0b1841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '_neutral'\n",
    "#if 'neutral' in exp_config['clf_debias']:\n",
    "#    suffix = '_neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342e9a7-438e-4397-ba69-8b18c5e5b809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recalls = [res['recall'] for res in results]\n",
    "counts, bins = np.histogram(recalls)\n",
    "plt.hist(bins[:-1], bins, weights=counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34084914-de53-46b6-a7f6-ce76a69bd400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blacklist_models = ['glove-wiki-gigaword-300', 'word2vec-google-news-300']\n",
    "blacklist_exp = []\n",
    "\n",
    "cur_model = \"\"\n",
    "for i, res in enumerate(results):\n",
    "    if exp_parameters[i]['embedder'] != cur_model:\n",
    "        cur_model = exp_parameters[i]['embedder']\n",
    "        print()\n",
    "        print(cur_model)\n",
    "    if res['recall'] > 0.6 and min(res['class_recall']) > 0.3:\n",
    "        continue\n",
    "    \n",
    "    if not cur_model in blacklist_models:\n",
    "        blacklist_models.append(cur_model)\n",
    "        blacklist_exp.append(i)\n",
    "    if exp_parameters[i]['debias']:\n",
    "        print(exp_parameters[i]['lr'], exp_parameters[i]['debias'], exp_parameters[i]['debias_k'])\n",
    "    else:\n",
    "        print(exp_parameters[i]['lr'], exp_parameters[i]['debias'])\n",
    "    #print(res['recall'])\n",
    "    #print(res['class_recall'])\n",
    "    \n",
    "blacklist_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7a0b2-4721-4061-a53f-60750a4e8cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.var(results[0]['WEAT_classwise'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db7ab8-3625-45a8-99e3-62c7a8a14574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_list = list(cosine_scores.keys())+['extrinsic']\n",
    "score_list.remove('gWEAT')\n",
    "scores_per_bias_type = {bt : {score: [] for score in score_list+['subgroup_AUC', 'BNSP', 'BPSN']} for bt in exp_config['bias_types']}\n",
    "#class_wise_scores = {score: [] for score in score_list}\n",
    "\n",
    "counts = {score: 0 for score in score_list}\n",
    "corr_per_score = {score: {'r': [], 'p': []} for score in score_list}\n",
    "for i in range(len(results)):\n",
    "    print(\"experiment\", i, \"with bias type\", exp_parameters[i]['bias_type'], \"and\", exp_parameters[i]['embedder'])\n",
    "\n",
    "#    if i in blacklist_exp:\n",
    "    if exp_parameters[i]['embedder'] in blacklist_models:\n",
    "        print(\"skip blacklisted models\")\n",
    "        continue\n",
    "    \n",
    "    for score in score_list:\n",
    "        #if score == 'SAME':\n",
    "        #scores_per_bias_type[exp_parameters[i]['bias_type']][score].append(np.mean(np.var(results[i][score+\"_classwise\"+suffix], axis=1)))\n",
    "        #else:\n",
    "        scores_per_bias_type[exp_parameters[i]['bias_type']][score].append(np.mean(results[i][score]))\n",
    "    for score in ['subgroup_AUC', 'BNSP', 'BPSN']:\n",
    "        scores_per_bias_type[exp_parameters[i]['bias_type']][score].append(np.mean(np.mean(np.abs(results[i][score]), axis=1)))\n",
    "    \n",
    "    for score in score_list:#+['BPSN']:\n",
    "        if score == 'gWEAT':\n",
    "            continue\n",
    "        for score2 in score_list:\n",
    "            if score == score2 or score2 == 'gWEAT':\n",
    "                continue\n",
    "                \n",
    "            # class-wise bias scores\n",
    "            if score == 'BPSN':\n",
    "                scores1 = np.mean(results[i][score], axis=0)\n",
    "            else:\n",
    "                scores1 = np.mean(results[i][score+'_classwise'+suffix], axis=0)\n",
    "            if score2 == 'DirectBias' or score2 == 'MAC':\n",
    "                scores1 = np.abs(scores1)\n",
    "            scores2 = np.mean(results[i][score2+'_classwise'+suffix], axis=0)\n",
    "            \n",
    "            #scores1 = list(itertools.chain.from_iterable(results[i][score+'_individual']))\n",
    "            #scores2 = list(itertools.chain.from_iterable(results[i][score2+'_individual']))\n",
    "            \n",
    "            #class_wise_scores[score].append(scores1)\n",
    "            try:\n",
    "                slope, intercept, r, p, std_err = scipy.stats.linregress(scores1, scores2)\n",
    "            except ValueError:\n",
    "                print(\"invalid values for \", score, score2)\n",
    "                print(results[i]['recall'])\n",
    "            if 'extrinsic' in score:\n",
    "                corr_per_score[score2]['r'].append(r)\n",
    "                corr_per_score[score2]['p'].append(p)\n",
    "                \n",
    "            if 'extrinsic' in score and p < 0.01 and np.abs(r) > 0.7:\n",
    "                print(score, score2, \"R=\"+str(r)+\" (p=\"+str(p)+\")\")\n",
    "                counts[score2] += 1\n",
    "                #res = {score: scores1, score2: scores2}\n",
    "                #df = pd.DataFrame(res)\n",
    "                #sns.regplot(x=score, y=score2, data=df).set_title(\"R=\"+str(r)+\" (p=\"+str(p)+\")\")\n",
    "                #plt.show()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d0818-1d6a-4c98-9651-21393b46d910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a93043-54ea-4f14-b9c2-1f8e88a1ab26",
   "metadata": {},
   "source": [
    "## Correlation of cosine scors with extrinsic bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b60d1d-8804-44e4-b81e-cfd862991241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_corr_per_score = {score: {'r': np.mean(corr_per_score[score]['r']), 'p': np.mean(corr_per_score[score]['p']), 'err': np.std(corr_per_score[score]['r'])} for score in ['SAME', 'WEAT', 'DirectBias', 'MAC']}\n",
    "mean_corr_per_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8d5a5-bca5-4c1d-a4eb-b1286add9a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_scores = ['SAME', 'WEAT', 'DirectBias']#, 'MAC']\n",
    "width = 0.5\n",
    "#offset = np.asarray([-3*width/2, -width/2, width/2, 3*width/2])\n",
    "x = np.arange(len(eval_scores))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "    \n",
    "for i, score in enumerate(eval_scores):\n",
    "    r_mean = mean_corr_per_score[score]['r']\n",
    "    r_std = mean_corr_per_score[score]['err']\n",
    "    p_mean = mean_corr_per_score[score]['p']\n",
    "    \n",
    "    ax.bar(x[i], r_mean, width, yerr=r_std)#, label=score)\n",
    "    \n",
    "ax.set_ylabel('Mean Pearson Correlation', fontsize=16)\n",
    "ax.set_xticks(x, eval_scores, fontsize=16)\n",
    "#ax.set_ylim(-0.19,1.1)\n",
    "ax.grid(color='grey', linestyle='--', axis='y')\n",
    "ax.set_title('Pearson Correlations of class-wise cosine scores with TP GAP', fontsize=20)\n",
    "#ax.legend(loc='upper right', bbox_to_anchor=(0.85, 0.5, 0., 0.5), fontsize=16)\n",
    "#plt.savefig('plots/word_bias_corr.eps', format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0d362-8d2f-4a61-87be-a35dbc7f9f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_axis_style(ax, labels):\n",
    "    ax.set_xticks(np.arange(1, len(labels) + 1), labels=labels)\n",
    "    ax.set_xlim(0.25, len(labels) + 0.75)\n",
    "    #ax.set_xlabel('Sample name')\n",
    "    ax.hlines(0,0.25, len(labels) + 0.75, 'grey', '--', linewidths=0.8)\n",
    "\n",
    "data = [corr_per_score[score]['r'] for score in eval_scores]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 9), sharey=True)\n",
    "\n",
    "# set style for the axes\n",
    "labels = eval_scores\n",
    "set_axis_style(ax, labels)\n",
    "\n",
    "if exp_config['clf_debias'] == 'no':\n",
    "    ax.set_title('Raw')\n",
    "elif 'neutral' in exp_config['clf_debias']:\n",
    "    ax.set_title('Gender-scrubbed')\n",
    "elif 'resample' in exp_config['clf_debias']:\n",
    "    ax.set_title('Resampled')\n",
    "#ax.set_title('Pearson Correlation with class-wise TP GAP')# of class-wise cosine scores with the TP GAP')\n",
    "ax.set_ylabel('Pearson Coefficient R')\n",
    "ax.violinplot(data)\n",
    "\n",
    "plt.subplots_adjust(bottom=0.15, wspace=0.05)\n",
    "plt.savefig('plots/bios_class_bias_'+exp_config['clf_debias']+'.png', bbox_inches=\"tight\")\n",
    "#plt.savefig('plots/bios_class_bias.eps', format='eps') # cant handle transparency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70179776-073e-4152-bfab-1d4f4d85036b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_scores = {}\n",
    "for bt, res in scores_per_bias_type.items():\n",
    "    mean_scores[bt] = {score: 0}\n",
    "    for score in score_list+['subgroup_AUC', 'BNSP', 'BPSN']:\n",
    "        mean_scores[bt][score] = np.abs(scores_per_bias_type[bt][score])\n",
    "        \n",
    "for comp in ['extrinsic']:#, 'subgroup_AUC', 'BNSP', 'BPSN']:\n",
    "    for bt, res in mean_scores.items():\n",
    "        df = pd.DataFrame(res)\n",
    "        print(bt)\n",
    "        for score in score_list:\n",
    "            if not score == 'extrinsic':\n",
    "                print(score)\n",
    "                slope, intercept, r, p, std_err = scipy.stats.linregress(df.loc[:,comp], df.loc[:,score])\n",
    "                #print(r, p)\n",
    "                sns.regplot(x=comp, y=score, data=df).set_title(\"R=\"+str(r)+\" (p=\"+str(p)+\")\")\n",
    "                plt.show()\n",
    "                \n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289304f-742e-4253-922b-ae6904379605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debias_ks = exp_config['debias_k']\n",
    "embedder = exp_config['embedders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e7acf-2a5c-4bde-b8eb-abd8046c9b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debias_ks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f19da-483f-4cc9-8a4d-5abc7f25379f",
   "metadata": {},
   "source": [
    "## Can we distinguish less/more biased models with cosine scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbcb875-314b-4043-aa48-e91dbb51b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if suffix == '_neutral':\n",
    "    for i in range(len(results)):\n",
    "        results[i]['extrinsic_neutral'] = [np.mean(fold_bias) for fold_bias in results[i]['extrinsic_classwise_neutral']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db742d36-5772-4f45-b887-61986c5a3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_low_bias = []\n",
    "exp_high_bias = []\n",
    "\n",
    "biases = [np.mean(results[i]['extrinsic'+suffix]) for i in range(len(results)) if exp_parameters[i]['embedder'] not in blacklist_models]\n",
    "valid_exp_ids = [i for i in range(len(results)) if exp_parameters[i]['embedder'] not in blacklist_models]\n",
    "counts, bins = np.histogram(biases)\n",
    "plt.hist(bins[:-1], bins, weights=counts)\n",
    "plt.show()\n",
    "\n",
    "mean = np.mean(biases)\n",
    "std = np.std(biases)\n",
    "for i in range(len(biases)):\n",
    "    if biases[i] < mean-std:\n",
    "        exp_low_bias.append(valid_exp_ids[i])\n",
    "    if biases[i] > mean+std:\n",
    "        exp_high_bias.append(valid_exp_ids[i])\n",
    "\n",
    "print(exp_low_bias)\n",
    "print(exp_high_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80450511-5ee8-4173-8cd4-bf86c7f6cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "def get_score_pred(exp_i, exp_j, score_key):\n",
    "    return int(np.mean(np.abs(results[exp_i][score_key])) < np.mean(np.abs(results[exp_j][score_key])))\n",
    "    \n",
    "extrinsic_pred = []\n",
    "for i in exp_low_bias:\n",
    "    for j in exp_high_bias:\n",
    "        extrinsic_pred.append(get_score_pred(i,j,'extrinsic'+suffix))\n",
    "        extrinsic_pred.append(get_score_pred(j,i,'extrinsic'+suffix))\n",
    "print(len(extrinsic_pred))\n",
    "\n",
    "models = []\n",
    "models2 = []\n",
    "for score in cosine_scores.keys():\n",
    "    \n",
    "    if score == 'gWEAT': # binary experiment\n",
    "        continue\n",
    "    print(score)\n",
    "    \n",
    "    score_pred = []\n",
    "    for i in exp_low_bias:\n",
    "        for j in exp_high_bias:\n",
    "            score_pred.append(get_score_pred(i,j,score+suffix))\n",
    "            score_pred.append(get_score_pred(j,i,score+suffix))\n",
    "    cm = confusion_matrix(extrinsic_pred, score_pred, normalize='true')\n",
    "    cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "    cm_display.ax_.get_images()[0].set_clim(0, 1)\n",
    "    cm_display.ax_.get_images()[0].set_cmap(plt.cm.Blues)\n",
    "    cm_display.ax_.set_title(score)\n",
    "    plt.savefig('plots/cm_bias_pred_'+score+'.png', bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"ROC AUC: \", roc_auc_score(extrinsic_pred, score_pred))\n",
    "    print(\"accuracy: \", accuracy_score(extrinsic_pred, score_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da46bce-d0c2-405e-a20a-2629b26093c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae62c6-85c3-4c00-bdd8-aea07ea6f908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
