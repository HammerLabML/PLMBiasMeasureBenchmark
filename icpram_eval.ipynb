{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import yaml\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "#from utils import create_balanced_occupation_data, create_occupation_data, occupation_stats\n",
    "from embedding import BertHuggingfaceMLM #, BertHuggingface\n",
    "from geometrical_bias import SAME, WEAT, GeneralizedWEAT, DirectBias, RIPA, MAC, normalize, cossim, EmbSetList, EmbSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '' # repository base\n",
    "RESULT_DIR = BASE_DIR+'results/icpram22/'\n",
    "EXP_CONFIG = RESULT_DIR+'config.yaml'\n",
    "PRETRAIN_BIAS_RES = 'task_res.csv'\n",
    "DATA_BIAS_RES = 'train_data_stats.csv'\n",
    "\n",
    "DATA_FILE = 'train_data.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXP_CONFIG, 'rb') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_DIR+config['template_file'], 'r') as f:\n",
    "    templates = yaml.safe_load(f)\n",
    "    \n",
    "attributes  = templates['protected_attr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_MODELS = config['iterations']*len(config['maxP'])*len(config['minP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-application",
   "metadata": {},
   "source": [
    "### Evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ['SEAT', 'MAC', 'DB', 'RIPA', 'GWEAT', 'cluster', 'neighbor', 'SVM']\n",
    "corr_per_measure = {'unmask': {'ETHNICITY': [], 'RELIGION': [], 'GENDER': []}}\n",
    "overall_biases = {'model-mean': {'ETHNICITY': [], 'RELIGION': [], 'GENDER': []}, 'model-var': {'ETHNICITY': [], 'RELIGION': [], 'GENDER': []},\n",
    "                  'data-mean': {'ETHNICITY': [], 'RELIGION': [], 'GENDER': []}, 'data-var': {'ETHNICITY': [], 'RELIGION': [], 'GENDER': []}}\n",
    "for score in scores:\n",
    "    corr_per_measure.update({score:{'ETHNICITY': [], 'RELIGION': [], 'GENDER': []}})\n",
    "    overall_biases.update({score: {'ETHNICITY': [], 'RELIGION': [], 'GENDER': []}})\n",
    "\n",
    "word_scores =  ['SEAT', 'DB', 'RIPA', 'unmask', 'data']\n",
    "score_corr = {}\n",
    "for attr in attributes:\n",
    "    score_corr.update({attr: {}})\n",
    "    for score in word_scores:\n",
    "        score_corr[attr].update({score: {}})\n",
    "        for score2 in word_scores:\n",
    "            score_corr[attr][score].update({score2: []})\n",
    "\n",
    "for test_model_id in range(NO_MODELS):\n",
    "    test_dir = RESULT_DIR+str(test_model_id)+\"/\"\n",
    "    \n",
    "    train_data_file =  RESULT_DIR+str(test_model_id)+\"/train_data.pickle\"\n",
    "    with open(train_data_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    data_bias_file = test_dir + DATA_BIAS_RES\n",
    "    pretrain_bias_file = test_dir + PRETRAIN_BIAS_RES\n",
    "    \n",
    "    df = pd.read_csv(data_bias_file)\n",
    "    df2 = pd.read_csv(pretrain_bias_file)\n",
    "\n",
    "    for attr in attributes:\n",
    "        groups = templates[attr][0]\n",
    "\n",
    "        p_equal = 1.0/len(groups)\n",
    "        mean_diff_data = np.mean([np.abs(np.asarray(df.loc[:,group])-np.ones(len(df.loc[:,group]))*p_equal) for group in groups], axis=0)\n",
    "        mean_diff_model = np.mean([np.abs(np.asarray(df2.loc[:,group])-np.ones(len(df2.loc[:,group]))*p_equal) for group in groups], axis=0)\n",
    "\n",
    "        dist_equal = [p_equal]*len(groups)\n",
    "        divergence_data = [scipy.spatial.distance.jensenshannon(dist_equal, list(df.loc[i,groups])) for i in range(len(df))]\n",
    "        divergence_model = [scipy.spatial.distance.jensenshannon(dist_equal, list(df2.loc[i,groups])) for i in range(len(df2))]\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(divergence_data, divergence_model)\n",
    "        model_r_value = r_value\n",
    "        \n",
    "        thresh = 0.0\n",
    "        if model_r_value >= thresh:\n",
    "            corr_per_measure['unmask'][attr].append(r_value)\n",
    "\n",
    "            overall_biases['model-mean'][attr].append(np.mean(divergence_model))\n",
    "            overall_biases['model-var'][attr].append(np.var(divergence_model))\n",
    "            overall_biases['data-mean'][attr].append(np.mean(divergence_data))\n",
    "            overall_biases['data-var'][attr].append(np.var(divergence_data))\n",
    "\n",
    "        for i, score in enumerate(scores):\n",
    "            score_name_in_dict = score\n",
    "            if score == 'DB':\n",
    "                score_name_in_dict = 'DirectBias'\n",
    "            elif score == 'SVM':\n",
    "                score_name_in_dict = 'classification'\n",
    "            elif score == 'SEAT':\n",
    "                score_name_in_dict = 'WEAT'\n",
    "            if score_name_in_dict+'_i_bias' in data.keys():\n",
    "                score_name_in_dict = score_name_in_dict+'_i'\n",
    "            df_score = data[score_name_in_dict+\"_bias\"]\n",
    "            \n",
    "            if len(groups) == 2:\n",
    "                # weat and same return signed scores but we compare with absolute values\n",
    "                score_biases = np.abs(df_score.loc[:,attr])\n",
    "            else:\n",
    "                score_biases = df_score.loc[:,attr]\n",
    "            \n",
    "            if model_r_value >= thresh:\n",
    "                slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(divergence_data, score_biases)\n",
    "                corr_per_measure[score][attr].append(r_value)\n",
    "            \n",
    "            # either use abs value or normalize weat scores to [0,1]\n",
    "            if not type(data['overall_biases'].loc[attr, score_name_in_dict]) == dict:\n",
    "                overall_biases[score][attr].append(np.abs(data['overall_biases'].loc[attr, score_name_in_dict]))\n",
    "                \n",
    "        # score-score correlation\n",
    "        for i, score in enumerate(word_scores[:-2]):\n",
    "            score_name_in_dict = score\n",
    "            if score == 'DB':\n",
    "                score_name_in_dict = 'DirectBias'\n",
    "            elif score == 'SVM':\n",
    "                score_name_in_dict = 'classification'\n",
    "            if score_name_in_dict+'_i_bias' in data.keys():\n",
    "                score_name_in_dict = score_name_in_dict+'_i'\n",
    "            elif score == 'SEAT':\n",
    "                score_name_in_dict = 'WEAT'\n",
    "            df_score = data[score_name_in_dict+\"_bias\"]\n",
    "            \n",
    "            if len(groups) == 2:\n",
    "                # weat and same return signed scores but we compare with absolute values\n",
    "                score_biases = np.abs(df_score.loc[:,attr])\n",
    "            else:\n",
    "                score_biases = df_score.loc[:,attr]\n",
    "            for j, score2 in enumerate(word_scores[:-1]): # model separately\n",
    "                if j > i:\n",
    "                    score_corr[attr][score][score2].append(float(\"nan\"))\n",
    "                elif j == i:\n",
    "                    score_corr[attr][score][score2].append(1)\n",
    "                else:\n",
    "                    score_name_in_dict2 = score2\n",
    "                    if score2 == 'DB':\n",
    "                        score_name_in_dict2 = 'DirectBias'\n",
    "                    elif score2 == 'SVM':\n",
    "                        score_name_in_dict2 = 'classification'\n",
    "                    if score_name_in_dict2+'_i_bias' in data.keys():\n",
    "                        score_name_in_dict2 = score_name_in_dict2+'_i'\n",
    "                    elif score2 == 'SEAT':\n",
    "                        score_name_in_dict2 = 'WEAT'\n",
    "                    df_score2 = data[score_name_in_dict2+\"_bias\"]\n",
    "\n",
    "                    if len(groups) == 2:\n",
    "                        score2_biases = np.abs(df_score2.loc[:,attr])\n",
    "                    else:\n",
    "                        score2_biases = df_score2.loc[:,attr]\n",
    "\n",
    "                    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(score_biases, score2_biases)\n",
    "                    score_corr[attr][score][score2].append(r_value)\n",
    "            slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(score_biases, divergence_model)\n",
    "            score_corr[attr]['unmask'][score].append(r_value)\n",
    "            slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(score_biases, divergence_data)\n",
    "            score_corr[attr]['data'][score].append(r_value)\n",
    "        \n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(divergence_model, divergence_data)\n",
    "        score_corr[attr]['data']['unmask'].append(r_value)\n",
    "        score_corr[attr]['unmask']['unmask'].append(1)\n",
    "        score_corr[attr]['data']['data'].append(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_corr['GENDER'].keys())\n",
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "for attr in attributes:\n",
    "    print(attr)\n",
    "    mean_score_corr = {}\n",
    "    for score in word_scores:\n",
    "        mean_score_corr.update({score: {}})\n",
    "        for score2 in word_scores:\n",
    "            mean_score_corr[score].update({score2: np.mean(score_corr[attr][score][score2])})\n",
    "\n",
    "    score_corr_df = pd.DataFrame(data=mean_score_corr)\n",
    "    \n",
    "    if attr == 'GENDER':\n",
    "        hm = sns.heatmap(score_corr_df, fmt=\".2f\", cmap='crest', annot=True, annot_kws={\"fontsize\":20})\n",
    "    else:\n",
    "        hm = sns.heatmap(score_corr_df.loc['DB':,score_corr_df.columns[1:]], fmt=\".2f\", cmap='crest', annot=True, annot_kws={\"fontsize\":20})\n",
    "    hm.axes.set_title(attr.title(), fontsize=30)#'Word Bias Score Correlations ('+attr+')', fontsize=30)\n",
    "    #hm.tick_params(labelsize=25)\n",
    "    hm.tick_params(labelsize=25, rotation=45)\n",
    "    plt.savefig('plots/word_score_heatmap_'+attr+'.eps', format='eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores = word_scores[:-1]\n",
    "width = 0.2\n",
    "offset = np.asarray([-3*width/2, -width/2, width/2, 3*width/2])\n",
    "x = np.arange(len(attributes))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "    \n",
    "for i, score in enumerate(eval_scores):\n",
    "    r2_mean = []\n",
    "    r2_std = []\n",
    "    \n",
    "    for attr in attributes:\n",
    "        r2_mean.append(np.mean(corr_per_measure[score][attr]))\n",
    "        r2_std.append(np.std(corr_per_measure[score][attr]))\n",
    "    \n",
    "    ax.bar(x+offset[i], r2_mean, width, yerr=r2_std, label=score)\n",
    "ax.set_ylabel('Pearson correlation', fontsize=20)\n",
    "ax.set_xticks(x, attributes, fontsize=16)\n",
    "ax.set_ylim(-0.19,1.1)\n",
    "ax.grid(color='grey', linestyle='--', axis='y')\n",
    "ax.set_title('Pearson Correlations with data biases', fontsize=25)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(0.85, 0.5, 0., 0.5), fontsize=16)\n",
    "plt.savefig('plots/word_bias_corr.eps', format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_eval_scores = ['cluster', 'SEAT', 'GWEAT', 'DB', 'RIPA', 'SVM', 'neighbor', 'unmask', 'data']\n",
    "width = 0.1\n",
    "offset = np.asarray([-7*width/2, -5*width/2, -3*width/2, -width/2, width/2, 3*width/2, 5*width/2, 7*width/2])\n",
    "x = np.arange(len(attributes))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "for i, score in enumerate(overall_eval_scores):\n",
    "    r2s_mean = []\n",
    "    ps_mean = []\n",
    "    \n",
    "    for attr in attributes:\n",
    "        \n",
    "        if not score in ['unmask','data']:\n",
    "            if len(overall_biases[score][attr]) == 0:\n",
    "                r2s_mean.append(0)\n",
    "                ps_mean.append(0)\n",
    "                continue\n",
    "\n",
    "            if len(overall_biases[score][attr]) != len(overall_biases['data-mean'][attr]):\n",
    "                r2s_mean.append(0)\n",
    "                ps_mean.append(0)\n",
    "                print(score)\n",
    "                continue\n",
    "            \n",
    "        if score == 'unmask':\n",
    "            slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(overall_biases['data-mean'][attr], overall_biases['model-mean'][attr])\n",
    "        elif score != 'data':\n",
    "            slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(overall_biases['data-mean'][attr], overall_biases[score][attr])\n",
    "        r2s_mean.append(r_value)\n",
    "        ps_mean.append(p_value)\n",
    "    \n",
    "    if score == 'data':\n",
    "        continue\n",
    "    ax.bar(x+offset[i], r2s_mean, width, yerr=ps_mean, label=score)\n",
    "    \n",
    "ax.set_ylabel('Pearson correlation', fontsize=20)\n",
    "ax.set_xticks(x, attributes, fontsize=16)\n",
    "ax.set_title('Pearson Correlations with mean data biases', fontsize=25)\n",
    "#ax.set_ylim(-1.0,1.0)\n",
    "xlim = ax.get_xlim()\n",
    "ax.set_xlim(xlim[0]+0.2,xlim[1]+0.5)\n",
    "ax.grid(color='grey', linestyle='--', axis='y')\n",
    "ax.legend(loc='upper right', fontsize=16)\n",
    "plt.savefig('plots/bias_corr.eps', format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_corr = {}\n",
    "for attr in attributes:\n",
    "    score_corr.update({attr: {}})\n",
    "    for score in overall_eval_scores:\n",
    "        score_corr[attr].update({score: {}})\n",
    "        for score2 in overall_eval_scores:\n",
    "            score_corr[attr][score].update({score2: 0})\n",
    "            \n",
    "for attr in attributes:\n",
    "    for i, score in enumerate(overall_eval_scores+['data']):\n",
    "        if score == 'unmask':\n",
    "            biases = overall_biases['model-mean'][attr]\n",
    "        elif score == 'data':\n",
    "            biases = overall_biases['data-mean'][attr]\n",
    "        else:\n",
    "            biases = overall_biases[score][attr]\n",
    "\n",
    "        for j, score2 in enumerate(overall_eval_scores+['data']):\n",
    "            #print(attr, score, score2)\n",
    "            if j > i:\n",
    "                score_corr[attr][score][score2] = float(\"nan\")\n",
    "            elif j == i:\n",
    "                score_corr[attr][score][score2] = 1.0\n",
    "            else:\n",
    "                if score2 == 'unmask':\n",
    "                    biases2 = overall_biases['model-mean'][attr]\n",
    "                elif score2 == 'data':\n",
    "                    biases2 = overall_biases['data-mean'][attr]\n",
    "                else:\n",
    "                    biases2 = overall_biases[score2][attr]\n",
    "\n",
    "                if len(biases) == 0 or len(biases2) == 0:\n",
    "                    score_corr[attr][score][score2] = float(\"nan\")\n",
    "                else:\n",
    "                    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(biases, biases2)\n",
    "                    score_corr[attr][score][score2] = r_value\n",
    "\n",
    "    score_corr_df = pd.DataFrame(data=score_corr[attr])\n",
    "    \n",
    "    if attr == 'GENDER':\n",
    "        hm = sns.heatmap(score_corr_df, fmt=\".2f\", cmap='crest', annot=True, annot_kws={\"size\":20})\n",
    "    else:\n",
    "        hm = sns.heatmap(score_corr_df.loc['GWEAT':, score_corr_df.columns[2:]], fmt=\".2f\", cmap='crest', annot=True, annot_kws={\"size\":20})\n",
    "    hm.axes.set_title(attr.title(), fontsize=30)\n",
    "    hm.tick_params(labelsize=25, rotation=45)\n",
    "    plt.savefig('plots/score_heatmap_'+attr+'.eps', format='eps', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-worse",
   "metadata": {},
   "source": [
    "## Robustness of word bias scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bias_scores = ['WEAT', 'DirectBias', 'RIPA', 'model JSD', 'data JSD', 'model', 'data']\n",
    "\n",
    "target_domain = templates['target']\n",
    "target_words = templates[target_domain]\n",
    "attributes = templates['protected_attr']\n",
    "\n",
    "iter_id = 0\n",
    "\n",
    "std_per_score = {}\n",
    "for score in word_bias_scores:\n",
    "    std_per_score.update({score: []})\n",
    "for minP in config['minP']:\n",
    "    for maxP in config['maxP']:\n",
    "        # all models ids that were trained on the same probability distribution created by minP,maxP\n",
    "        iter_ids = range(iter_id, iter_id+config['iterations'])\n",
    "        \n",
    "        for score in word_bias_scores:\n",
    "            #print(score)\n",
    "            biases = []\n",
    "            for model_id in iter_ids:\n",
    "                test_dir = RESULT_DIR+str(model_id)+\"/\"\n",
    "                train_data_file =  RESULT_DIR+str(model_id)+\"/train_data.pickle\"\n",
    "                with open(train_data_file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "\n",
    "                data_bias_file = test_dir + DATA_BIAS_RES\n",
    "                pretrain_bias_file = test_dir + PRETRAIN_BIAS_RES\n",
    "\n",
    "                if 'model' in score:\n",
    "                    df = pd.read_csv(data_bias_file)\n",
    "                    df = df.loc[:,df.columns[1:]] # removing the column with job titles\n",
    "                elif 'data' in score:\n",
    "                    df = pd.read_csv(pretrain_bias_file)\n",
    "                    df = df.loc[:,df.columns[1:]] # removing the column with job titles\n",
    "                else:\n",
    "                    df = data[score+'_bias']\n",
    "\n",
    "                if 'JSD' in score:\n",
    "                    bias_by_attr_target = {}\n",
    "                    for attr in attributes:\n",
    "                        bias_by_attr_target.update({attr: {}})\n",
    "                        for target in target_words:\n",
    "                            bias_by_attr_target[attr].update({target: 0})\n",
    "                    for attr in attributes:\n",
    "                        groups = templates[attr][0]\n",
    "                        p_equal = 1.0/len(groups)\n",
    "                        dist_equal = [p_equal]*len(groups)\n",
    "                        divergence = [scipy.spatial.distance.jensenshannon(dist_equal, list(df.loc[i,groups])) for i in range(len(df))]\n",
    "                        #print(divergence)\n",
    "                        for i, target in enumerate(target_words):\n",
    "                            bias_by_attr_target[attr][target] = divergence[i]\n",
    "\n",
    "                    df = pd.DataFrame(data=bias_by_attr_target)\n",
    "                    \n",
    "                biases.append(df.to_numpy())\n",
    "            \n",
    "            # standard deviation of biases between the 5 models, mean over all targets\n",
    "            pdiffs = []\n",
    "            for idx in range(config['iterations']-1):\n",
    "                pdiffs.append(abs(biases[idx]-biases[0])/biases[0])\n",
    "            #print(pdiffs)\n",
    "            #print(biases)\n",
    "            std_per_score[score].append(np.mean(np.std(pdiffs, axis=0), axis=0))\n",
    "        \n",
    "        iter_id = iter_ids[-1]+1 \n",
    "        \n",
    "for score in word_bias_scores:\n",
    "    print(score, np.mean(np.vstack(std_per_score[score]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in attributes:\n",
    "    count = 0\n",
    "    for temp in templates['templates_train']:\n",
    "        if attr in temp:\n",
    "            count += 1\n",
    "    print(\"found\", count, \"training templates for attr\", attr)\n",
    "    \n",
    "    count = 0\n",
    "    for temp in templates['templates_test']:\n",
    "        if attr in temp:\n",
    "            count += 1\n",
    "    print(\"found\", count, \"test templates for attr\", attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(templates['templates_train']))\n",
    "print(len(templates['templates_test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-klein",
   "metadata": {},
   "source": [
    "## Permutation test to measure template influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_model_id)\n",
    "test_dir = RESULT_DIR+str(test_model_id)+\"/\"\n",
    "\n",
    "data_bias_file = test_dir + DATA_BIAS_RES\n",
    "pretrain_bias_file = test_dir + PRETRAIN_BIAS_RES\n",
    "\n",
    "df = pd.read_csv(data_bias_file)\n",
    "df2 = pd.read_csv(pretrain_bias_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmasking_bias(unmasker, masked_sent, group_tokens):\n",
    "    result = unmasker(masked_sent, targets=group_tokens, top_k=len(group_tokens))\n",
    "\n",
    "    prob = 0\n",
    "    for res in result:\n",
    "        prob += res['score']\n",
    "\n",
    "    probs = []\n",
    "    for token in group_tokens:\n",
    "        for res in result:\n",
    "            if res['token_str'] == token:\n",
    "                probs.append(res['score'] / prob)\n",
    "    return probs\n",
    "\n",
    "def unmasking_bias_multi_attr(bert, template_config, target_words):\n",
    "    count = 0\n",
    "    probabilities = []\n",
    "    masked_sentences = []\n",
    "    attr_label = []\n",
    "    target_label = []\n",
    "    attributes = tmp['protected_attr']\n",
    "    templates = tmp['templates_test']\n",
    "\n",
    "    group_token_by_attr = {}\n",
    "    attr_results = {}\n",
    "    for attr in attributes:\n",
    "        group_token_by_attr.update({attr: []})\n",
    "        attr_results.update({attr: {}})\n",
    "        for i in range(len(template_config[attr])):\n",
    "            group_token_by_attr[attr].append(template_config[attr][i])\n",
    "\n",
    "    probs_by_target_group = {}\n",
    "    for group in groups:\n",
    "        probs_by_target_group.update({group: {}})\n",
    "        for target in target_words:\n",
    "            probs_by_target_group[group].update({target: []})\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        unmasker = pipeline('fill-mask', model=bert.model, tokenizer=bert.tokenizer, device=0)\n",
    "    else:\n",
    "        unmasker = pipeline('fill-mask', model=bert.model, tokenizer=bert.tokenizer, device=-1)\n",
    "\n",
    "    for temp in templates:\n",
    "        for attr in attributes:\n",
    "            # count back in case there are more than 10 versions of this attribute (e.g. GENDER10 contains GENDER1)\n",
    "            sent = temp\n",
    "\n",
    "            # replace all other attributes with the neutral term\n",
    "            for attr2 in attributes:\n",
    "                if attr2 == attr:\n",
    "                    continue\n",
    "                for i in range(len(template_config[attr2]) - 1, -1, -1):\n",
    "                    cur_attr = attr2 + str(i)\n",
    "                    sent = sent.replace(cur_attr, template_config[attr2 + '_neutral'][i])\n",
    "\n",
    "            # now insert the mask for the targeted attribute\n",
    "            for i in range(len(template_config[attr]) - 1, -1, -1):\n",
    "                cur_attr = attr + str(i)\n",
    "                if cur_attr not in sent:\n",
    "                    continue\n",
    "                sent2 = sent\n",
    "\n",
    "                sent2 = sent2.replace(cur_attr, '[MASK]')\n",
    "                # in case there are multiple words defining this attribute, replace others with the neutral term\n",
    "                for j in range(len(template_config[attr]) - 1, -1, -1):\n",
    "                    if not j == i:\n",
    "                        sent2 = sent2.replace(attr + str(j), template_config[attr + '_neutral'][j])\n",
    "\n",
    "                # replace target and obtain unmasking probabilities for each group per target\n",
    "                for target in target_words:\n",
    "                    masked_sent = sent2.replace(template_config['target'], target)\n",
    "\n",
    "                    if not masked_sent.count('[MASK]') == 1:\n",
    "                        print(\"zero or mulitple masks in sentence!\")\n",
    "                        print(masked_sent)\n",
    "                        print(sent)\n",
    "                        print(cur_attr)\n",
    "                    probs = unmasking_bias(unmasker, masked_sent, group_token_by_attr[attr][i])\n",
    "                    masked_sentences.append(masked_sent)\n",
    "                    attr_label.append(attr)\n",
    "                    target_label.append(target)\n",
    "                    probabilities.append(probs)\n",
    "\n",
    "                # if there are other versions of this attribute, this will be replaced with the neutral term anyways\n",
    "                sent = sent.replace(cur_attr, template_config[attr + '_neutral'][i])\n",
    "\n",
    "    probs_by_attr_target = {}\n",
    "    for attr in attributes:\n",
    "        probs_by_attr_target.update({attr: {}})\n",
    "        for target in target_words:\n",
    "            probs_by_attr_target[attr].update({target: []})\n",
    "            \n",
    "    sent_by_attr_target = {}\n",
    "    for attr in attributes:\n",
    "        sent_by_attr_target.update({attr: {}})\n",
    "        for target in target_words:\n",
    "            sent_by_attr_target[attr].update({target: []})\n",
    "            \n",
    "    for i, prob in enumerate(probabilities):\n",
    "        probs_by_attr_target[attr_label[i]][target_label[i]].append(prob)\n",
    "        sent_by_attr_target[attr_label[i]][target_label[i]].append(masked_sentences[i])\n",
    "        \n",
    "    return probs_by_attr_target, sent_by_attr_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "\n",
    "def template_permutation_test(bert, tmp, target_words, n_iter=1000):\n",
    "    \n",
    "    protected_attributes = tmp['protected_attr']\n",
    "    groups = []\n",
    "    for attr in protected_attributes:\n",
    "        for group in tmp[attr][0]:\n",
    "            groups.append(group)\n",
    "    \n",
    "    protected_groups = {}\n",
    "    for attr in protected_attributes:\n",
    "        protected_groups.update({attr: tmp[attr][0]})\n",
    "    \n",
    "    probs_by_attr_target, sent_by_attr_target = unmasking_bias_multi_attr(bert, tmp, target_words)\n",
    "    \n",
    "    emb_by_attr_target = {}\n",
    "    for attr in attributes:\n",
    "        emb_by_attr_target.update({attr: {}})\n",
    "        for target in target_words:\n",
    "            emb_by_attr_target[attr].update({target: bert.embed(sent_by_attr_target[attr][target])})    \n",
    "    \n",
    "    mean_probs_by_attr_target = {}\n",
    "    std_probs_by_attr_target = {}\n",
    "    jsd_by_attr_target = {}\n",
    "    stds = []\n",
    "    for attr in probs_by_attr_target.keys():\n",
    "        mean_probs_by_attr_target.update({attr: {}})\n",
    "        std_probs_by_attr_target.update({attr: {}})\n",
    "        jsd_by_attr_target.update({attr: {}})\n",
    "\n",
    "        n_groups = len(probs_by_attr_target[attr][target_words[0]][0])\n",
    "        dist_equal = [1.0/n_groups]*n_groups\n",
    "\n",
    "        for target in target_words:\n",
    "            mean_probs_by_attr_target[attr].update({target: np.mean(probs_by_attr_target[attr][target], axis=0)})\n",
    "            std_probs_by_attr_target[attr].update({target: np.std(probs_by_attr_target[attr][target], axis=0)})\n",
    "            jsd_by_attr_target[attr].update({target: scipy.spatial.distance.jensenshannon(dist_equal, np.mean(probs_by_attr_target[attr][target], axis=0))})\n",
    "            stds += list(np.std(probs_by_attr_target[attr][target], axis=0))\n",
    "    \n",
    "    \n",
    "    attr_results = dict(zip(attributes,[{'pval': 0, 'pdiff': 0, 'jsd_diff:': 0, 'mean_std': np.mean(stds)} for attr in attributes]))\n",
    "    \n",
    "    for attr in attributes:\n",
    "        pdiff = []\n",
    "        jsd_diff = []\n",
    "        n = len(probs_by_attr_target[attr][target_words[0]])\n",
    "        n_groups = len(probs_by_attr_target[attr][target_words[0]][0])\n",
    "        dist_equal = [1.0/n_groups]*n_groups\n",
    "        n_samples = int(9*n/10)\n",
    "\n",
    "        p_val = 0\n",
    "        for it in range(n_iter):\n",
    "            idxs = np.random.permutation(n)\n",
    "\n",
    "            for target in target_words:\n",
    "                # unmasking biases\n",
    "                pat = [probs_by_attr_target[attr][target][i] for i in range(n) if i in idxs[:n_samples]]\n",
    "                mean_pat_it = np.mean(pat, axis=0)\n",
    "\n",
    "                mean_pat = mean_probs_by_attr_target[attr][target]\n",
    "                if not np.array_equal(np.argsort(mean_pat), np.argsort(mean_pat_it)):\n",
    "                    #print(np.argsort(mean_pat), np.argsort(mean_pat_it))\n",
    "                    p_val += 1\n",
    "                for i in range(len(mean_pat_it)):\n",
    "                    pdiff.append(np.abs(mean_pat_it[i]-mean_pat[i])/mean_pat[i])\n",
    "\n",
    "                jsd_it = scipy.spatial.distance.jensenshannon(dist_equal, mean_pat_it)\n",
    "                jsd_diff.append(np.abs(jsd_it-jsd_by_attr_target[attr][target])/jsd_by_attr_target[attr][target])\n",
    "                \n",
    "\n",
    "        p_val /= n_iter*len(target_words)\n",
    "        attr_results[attr]['pval'] = p_val\n",
    "        attr_results[attr]['pdiff'] = pdiff\n",
    "        attr_results[attr]['jsd_diff'] = jsd_diff\n",
    "\n",
    "    return attr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_DIR+config['template_file'], 'r') as f:\n",
    "    tmp = yaml.safe_load(f)\n",
    "    \n",
    "target_domain = tmp['target']\n",
    "target_words = tmp[target_domain]\n",
    "bert = BertHuggingfaceMLM(model_name=config['pretrained_model'], batch_size=config['batch_size'])\n",
    "\n",
    "results = []\n",
    "overall_bias_pdiffs = []\n",
    "word_bias_pdifffs = []\n",
    "for test_model_id in range(NO_MODELS):\n",
    "    print(\"############ At Model \", test_model_id, \"############\")\n",
    "    print()\n",
    "    test_dir = RESULT_DIR+str(test_model_id)+\"/\"\n",
    "\n",
    "    train_data_file =  RESULT_DIR+str(test_model_id)+\"/train_data.pickle\"\n",
    "    model_path =  RESULT_DIR+str(test_model_id)+\"/model/\"\n",
    "    with open(train_data_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    bert.load(model_path)\n",
    "    \n",
    "    attr_results = template_permutation_test(bert, tmp, target_words, n_iter=n_iter)\n",
    "    print(attr_results)\n",
    "    results.append(attr_results)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in attributes:\n",
    "    pvals = []\n",
    "    for res in results:\n",
    "        pvals.append(res[attr]['pval'])\n",
    "    print(attr, np.mean(pvals), np.std(pvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-offering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
