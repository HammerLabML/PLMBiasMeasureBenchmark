{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704a4ef-003a-48f0-af70-d63e34cb54b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, RocCurveDisplay, ConfusionMatrixDisplay\n",
    "\n",
    "from embedding import BertHuggingface\n",
    "from geometrical_bias import SAME, WEAT, GeneralizedWEAT, DirectBias, MAC, normalize, cossim, EmbSetList, EmbSet, GeometricBias\n",
    "from utils import CLFHead, SimpleCLFHead, CustomModel, JigsawDataset, BiosDataset, DebiasPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33eb3d3-7dfe-4603-86eb-e210ba735ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"jigsaw_unintended_bias\", data_dir=\"../../data/jigsaw_bias/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50bbf3d-7afa-4d55-8c52-c1ca51d2271d",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- check if debias reduces extrinsic biases\n",
    "- plot correlation (job-wise vs. overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e003c41-88a4-4853-b56e-1d482341f61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/protected_groups.json', 'r') as f:\n",
    "    pg_config = json.load(f)\n",
    "    \n",
    "with open('results/jigsaw_20343/config.json', 'r') as f:\n",
    "    exp_config = json.load(f)\n",
    "    \n",
    "#with open(exp_config['batch_size_lookup'], 'r') as f:\n",
    "#    batch_size_lookup = json.load(f)\n",
    "    \n",
    "groups_by_bias_types = pg_config['groups_by_bias_types']\n",
    "terms_by_groups = pg_config['terms_by_groups']\n",
    "\n",
    "cosine_scores = {'SAME': SAME, 'WEAT': WEAT, 'gWEAT': GeneralizedWEAT, 'DirectBias': DirectBias, 'MAC': MAC}\n",
    "optimizer = {'RMSprop': torch.optim.RMSprop, 'Adam': torch.optim.Adam}\n",
    "criterions = {'BCEWithLogitsLosss': torch.nn.BCEWithLogitsLoss, 'MultiLabelSoftMarginLoss': torch.nn.MultiLabelSoftMarginLoss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8031c-b5c0-4ba1-b796-f7cd4ae89ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246ead7-9c95-450f-adcc-b4501da08a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = exp_config['save_dir']+'res.pickle'\n",
    "with open(save_file, 'rb') as handle:\n",
    "    res = pickle.load(handle)\n",
    "    exp_parameters = res['params']\n",
    "    results = res['results']\n",
    "    #results_test = res['results_eval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c025214-e56c-49b1-8f57-6db35be9e7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(exp_parameters) == len(results), \"shape mismatch: \"+str(len(exp_parameters))+\" vs. \"+str(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94a580-fd4c-44ff-bb8c-b44cd27a1f68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recalls = [res['recall'] for res in results]\n",
    "precisions = [res['precision'] for res in results]\n",
    "counts, bins = np.histogram(recalls)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,5))\n",
    "axes[0].hist(bins[:-1], bins, weights=counts)\n",
    "axes[0].set_xlim(-0.05, 1.05)\n",
    "axes[0].set_title('Recall')\n",
    "\n",
    "counts, bins = np.histogram(precisions)\n",
    "axes[1].hist(bins[:-1], bins, weights=counts)\n",
    "axes[1].set_xlim(-0.05, 1.05)\n",
    "axes[1].set_title('Precision')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e8b2d-128d-4b8a-9a2d-03c882baa9ef",
   "metadata": {},
   "source": [
    "### Exclude models with bad performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c8cce-2faa-4b84-956b-bddb6ce24e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blacklist_models = []\n",
    "\n",
    "cur_model = \"\"\n",
    "for i, res in enumerate(results):\n",
    "    if exp_parameters[i]['embedder'] != cur_model:\n",
    "        cur_model = exp_parameters[i]['embedder']\n",
    "        print()\n",
    "        print(cur_model)\n",
    "    if res['recall'] > 0.3 and res['precision'] > 0.3:\n",
    "        continue\n",
    "    \n",
    "    if not cur_model in blacklist_models:\n",
    "        blacklist_models.append(cur_model)\n",
    "    if exp_parameters[i]['debias']:\n",
    "        print(exp_parameters[i]['lr'], exp_parameters[i]['debias'], exp_parameters[i]['debias_k'])\n",
    "    else:\n",
    "        print(exp_parameters[i]['lr'], exp_parameters[i]['debias'])\n",
    "    \n",
    "blacklist_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219971f-bb8c-4bdd-8580-1504ba98bc70",
   "metadata": {},
   "source": [
    "## Correlation of TP/TN GAP with cosine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123adf8a-b099-4cf4-98ac-747c157c84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_binary = ['SAME', 'WEAT', 'gWEAT', 'DirectBias', 'MAC']\n",
    "scores_multi = ['SAME', 'gWEAT', 'DirectBias', 'MAC']\n",
    "eval_scores = ['TP', 'TN', 'subgroup_AUC', 'BNSP', 'BPSN']\n",
    "\n",
    "scores_per_bias_type = {bt : {} for bt in exp_config['bias_types']}\n",
    "for bt in exp_config['bias_types']:\n",
    "    score_list = scores_multi+eval_scores\n",
    "    if bt == 'gender':\n",
    "        score_list = scores_binary+eval_scores\n",
    "    scores_per_bias_type[bt] = {score: [] for score in score_list}\n",
    "\n",
    "\n",
    "scores_per_bias_type2 = {bt : {} for bt in exp_config['bias_types']}\n",
    "for bt in exp_config['bias_types']:\n",
    "    score_list = scores_multi+eval_scores\n",
    "    if bt == 'gender':\n",
    "        score_list = scores_binary+eval_scores\n",
    "    scores_per_bias_type2[bt] = {score: [] for score in score_list}\n",
    "\n",
    "\n",
    "for i in range(len(results)):\n",
    "    print(\"experiment\", i, \"with bias type\", exp_parameters[i]['bias_type'], \"and\", exp_parameters[i]['embedder'])\n",
    "    \n",
    "    if exp_parameters[i]['embedder'] in blacklist_models+['glove-wiki-gigaword-300']:\n",
    "        print(\"skip blastlisted models\")\n",
    "        continue\n",
    "\n",
    "    gap_worked = True\n",
    "    for fold_res in results[i]['extrinsic_classwise']:\n",
    "        if not len(fold_res) == 2:\n",
    "            gap_worked = False\n",
    "            break\n",
    "\n",
    "    if not gap_worked:\n",
    "        continue\n",
    "\n",
    "    bt = exp_parameters[i]['bias_type']\n",
    "    for score in score_list:\n",
    "        if score == 'TN':\n",
    "            scores = [fold[0] for fold in results[i]['extrinsic_classwise']]\n",
    "            scores_per_bias_type[bt][score].append(scores)\n",
    "            scores_per_bias_type2[bt][score].append(np.mean(scores))\n",
    "        elif score == 'TP':\n",
    "            scores = [fold[1] for fold in results[i]['extrinsic_classwise']]\n",
    "            scores_per_bias_type[bt][score].append(scores)\n",
    "            scores_per_bias_type2[bt][score].append(np.mean(scores))\n",
    "        else:\n",
    "            scores_per_bias_type[bt][score].append(results[i][score])\n",
    "            scores_per_bias_type2[bt][score].append(np.mean(results[i][score]))\n",
    "    print(results[i]['extrinsic_classwise'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4ddb1-1c97-4c53-84d4-d08bb9117a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = {}\n",
    "for bt, res in scores_per_bias_type2.items():\n",
    "    mean_scores[bt] = {score: 0}\n",
    "    for score in score_list+['subgroup_AUC', 'BNSP', 'BPSN']:\n",
    "        mean_scores[bt][score] = np.abs(scores_per_bias_type2[bt][score])\n",
    "        \n",
    "for comp in ['TP', 'TN']:#, 'subgroup_AUC', 'BNSP', 'BPSN']:\n",
    "    for bt, res in mean_scores.items():\n",
    "        df = pd.DataFrame(res)\n",
    "        print(bt)\n",
    "        for score in score_list:\n",
    "            if not score in ['TP', 'TN']:\n",
    "                print(score)\n",
    "                slope, intercept, r, p, std_err = scipy.stats.linregress(df.loc[:,comp], df.loc[:,score])\n",
    "                #print(r, p)\n",
    "                sns.regplot(x=comp, y=score, data=df).set_title(\"R=\"+str(r)+\" (p=\"+str(p)+\")\")\n",
    "                plt.show()\n",
    "                \n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec55ada-300e-4d09-b004-2ac517f49ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for bt, res in scores_per_bias_type.items():\n",
    "    print(bt)\n",
    "\n",
    "    tp_res = np.asarray(res['TP'])\n",
    "    tn_res = np.asarray(res['TN'])\n",
    "    tp_res = np.hstack(tp_res)\n",
    "    tn_res = np.hstack(tn_res)\n",
    "    print(tp_res.shape)\n",
    "    for score in score_list[:4]: # only cosine scores\n",
    "        score_res = np.asarray(res[score])\n",
    "\n",
    "        score_res = np.hstack(score_res)\n",
    "\n",
    "        df = pd.DataFrame({'TP': tp_res, 'TN': tn_res, score: score_res})\n",
    "        \n",
    "        print(score)\n",
    "        slope, intercept, r, p, std_err = scipy.stats.linregress(tp_res, score_res)\n",
    "        sns.regplot(x='TP', y=score, data=df).set_title(\"TP: R=\"+str(r)+\" (p=\"+str(p)+\")\")\n",
    "        plt.show()\n",
    "        \n",
    "        slope, intercept, r, p, std_err = scipy.stats.linregress(tn_res, score_res)\n",
    "        sns.regplot(x='TN', y=score, data=df).set_title(\"TN: R=\"+str(r)+\" (p=\"+str(p)+\")\")\n",
    "        plt.show()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a63ac-02d1-4b21-a674-4a763b3e6880",
   "metadata": {},
   "source": [
    "## Can we distinguish less/more biased models with cosine scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc2bea-19e2-4f1c-bbb1-bd6537789ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = []\n",
    "auc_biases = []\n",
    "valid_exp_ids = []\n",
    "for i in range(len(results)):\n",
    "    if exp_parameters[i]['embedder'] not in blacklist_models:\n",
    "        class_biases = results[i]['extrinsic_classwise_neutral']\n",
    "        valid = True\n",
    "        for split in class_biases:\n",
    "            if len(split) != 2:\n",
    "                valid = False\n",
    "        if not valid:\n",
    "            continue\n",
    "\n",
    "        biases.append(np.mean(class_biases, axis=0))\n",
    "        auc_biases.append(np.mean(results[0]['subgroup_AUC']))\n",
    "        valid_exp_ids.append(i)\n",
    "\n",
    "biases = np.asarray(biases)\n",
    "auc_biases = np.asarray(auc_biases)\n",
    "\n",
    "print(\"TN:\")\n",
    "counts, bins = np.histogram(biases[:,0])\n",
    "plt.hist(bins[:-1], bins, weights=counts)\n",
    "plt.show()\n",
    "\n",
    "print(\"TP:\")\n",
    "counts, bins = np.histogram(biases[:,1])\n",
    "plt.hist(bins[:-1], bins, weights=counts)\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC:\")\n",
    "counts, bins = np.histogram(auc_biases)\n",
    "plt.hist(bins[:-1], bins, weights=counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cb734-84ec-4db2-a01e-c88d7aa3b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_low_bias = {'TP': [], 'TN': [], 'AUC': []}\n",
    "exp_high_bias = {'TP': [], 'TN': [], 'AUC': []}\n",
    "\n",
    "mean = np.mean(biases, axis=0)\n",
    "std = np.std(biases, axis=0)\n",
    "\n",
    "mean_auc = np.mean(auc_biases, axis=0)\n",
    "std_auc = np.std(auc_biases, axis=0)\n",
    "print(mean.shape)\n",
    "for i in range(len(biases)):\n",
    "    if biases[i,0] < (mean-std)[0]:\n",
    "        exp_low_bias['TN'].append(valid_exp_ids[i])\n",
    "    if biases[i,0] > (mean+std)[0]:\n",
    "        exp_high_bias['TN'].append(valid_exp_ids[i])\n",
    "    \n",
    "    if biases[i,1] < (mean-std)[1]:\n",
    "        exp_low_bias['TP'].append(valid_exp_ids[i])\n",
    "    if biases[i,1] > (mean+std)[1]:\n",
    "        exp_high_bias['TP'].append(valid_exp_ids[i])\n",
    "\n",
    "print(exp_low_bias)\n",
    "print(exp_high_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee879b-2506-4eb8-9ff9-51d820cfda37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "def get_score_pred(exp_i, exp_j, score_key, idx=None):\n",
    "    if idx is None:\n",
    "        return int(results[exp_i][score_key] < results[exp_j][score_key])\n",
    "    else:\n",
    "        return int(np.mean(np.asarray(results[exp_i][score_key])[:,idx]) < np.mean(np.asarray(results[exp_j][score_key])[:,idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f278d9-0594-458a-8d89-d6b2693f28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True positive\n",
    "extrinsic_pred = []\n",
    "for i in exp_low_bias['TP']:\n",
    "    for j in exp_high_bias['TP']:\n",
    "        extrinsic_pred.append(get_score_pred(i,j,'extrinsic_classwise_neutral', 1))\n",
    "for i in exp_high_bias['TP']:\n",
    "    for j in exp_low_bias['TP']:\n",
    "        extrinsic_pred.append(get_score_pred(i,j,'extrinsic_classwise_neutral', 1))\n",
    "\n",
    "for score in cosine_scores.keys():\n",
    "    \n",
    "    if score == 'gWEAT' and exp_parameters[0]['bias_type'] == 'gender': # binary experiment\n",
    "        continue\n",
    "    if score == 'WEAT' and not exp_parameters[0]['bias_type'] == 'gender': # multi group experiment\n",
    "        continue\n",
    "    print(score)\n",
    "\n",
    "    score_pred = []\n",
    "    for i in exp_low_bias['TP']:\n",
    "        for j in exp_high_bias['TP']:\n",
    "            score_pred.append(get_score_pred(i,j,score+'_neutral'))\n",
    "    for i in exp_high_bias['TP']:\n",
    "        for j in exp_low_bias['TP']:\n",
    "            score_pred.append(get_score_pred(i,j,score+'_neutral'))\n",
    "\n",
    "    print(score_pred)\n",
    "    cm = confusion_matrix(extrinsic_pred, score_pred, normalize='true')\n",
    "    cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "    cm_display.ax_.get_images()[0].set_clim(0, 1)\n",
    "    cm_display.ax_.get_images()[0].set_cmap(plt.cm.Blues)\n",
    "    cm_display.ax_.set_title(score)\n",
    "    plt.savefig('plots/cm_bias_pred_'+score+'.png', bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"ROC AUC: \", roc_auc_score(extrinsic_pred, score_pred))\n",
    "    print(\"accuracy: \", accuracy_score(extrinsic_pred, score_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18bd36-f5df-4cc5-8cd2-a7c7b303b0c9",
   "metadata": {},
   "source": [
    "### True negative predictability was not used in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eef1dd-a990-46bc-b9f9-c23c4e06a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True negative\n",
    "extrinsic_pred = []\n",
    "for i in exp_low_bias['TN']:\n",
    "    for j in exp_high_bias['TN']:\n",
    "        extrinsic_pred.append(get_score_pred(i,j,'extrinsic_classwise_neutral', 0))\n",
    "for i in exp_high_bias['TN']:\n",
    "    for j in exp_low_bias['TN']:\n",
    "        extrinsic_pred.append(get_score_pred(i,j,'extrinsic_classwise_neutral', 0))\n",
    "\n",
    "for score in cosine_scores.keys():\n",
    "    \n",
    "    if score == 'gWEAT' and exp_parameters[0]['bias_type'] == 'gender': # binary experiment\n",
    "        continue\n",
    "    if score == 'WEAT' and not exp_parameters[0]['bias_type'] == 'gender': # multi group experiment\n",
    "        continue\n",
    "    print(score)\n",
    "\n",
    "    score_pred = []\n",
    "    for i in exp_low_bias['TN']:\n",
    "        for j in exp_high_bias['TN']:\n",
    "            score_pred.append(get_score_pred(i,j,score+'_neutral'))\n",
    "    for i in exp_high_bias['TN']:\n",
    "        for j in exp_low_bias['TN']:\n",
    "            score_pred.append(get_score_pred(i,j,score+'_neutral'))\n",
    "\n",
    "    print(score_pred)\n",
    "    cm = confusion_matrix(extrinsic_pred, score_pred, normalize='true')\n",
    "    cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "    cm_display.ax_.get_images()[0].set_clim(0, 1)\n",
    "    cm_display.ax_.get_images()[0].set_cmap(plt.cm.Blues)\n",
    "    cm_display.ax_.set_title(score)\n",
    "    plt.savefig('plots/cm_bias_pred_'+score+'.png', bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"ROC AUC: \", roc_auc_score(extrinsic_pred, score_pred))\n",
    "    print(\"accuracy: \", accuracy_score(extrinsic_pred, score_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca5ada-0253-492b-8ead-3e0a5e306169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
