{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52ba16-9852-4cc9-a564-7282f85a3616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "import difflib\n",
    "import string\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from embedding import BertHuggingfaceMLM\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from geometrical_bias import SAME, WEAT, GeneralizedWEAT, DirectBias, RIPA, MAC, normalize, cossim, EmbSetList, EmbSet, GeometricBias\n",
    "from unmasking_bias import PLLBias\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, RocCurveDisplay, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "from utils import CLFHead, SimpleCLFHead, CustomModel, CrowSPairsDataset, JigsawDataset, BiosDataset, MLMPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c8c2d-3135-4f09-b834-4bdc2ca65859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/protected_groups.json', 'r') as f:\n",
    "    pg_config = json.load(f)\n",
    "    \n",
    "with open('data/batch_size_lookup_1080.json', 'r') as f:\n",
    "    batch_size_lookup = json.load(f)\n",
    "    \n",
    "with open('results/mlm_20340/config.json', 'r') as f:\n",
    "    exp_config = json.load(f)\n",
    "    \n",
    "groups_by_bias_types = pg_config['groups_by_bias_types']\n",
    "terms_by_groups = pg_config['terms_by_groups']\n",
    "\n",
    "cosine_scores = {'SAME': SAME, 'WEAT': WEAT, 'gWEAT': GeneralizedWEAT, 'DirectBias': DirectBias}#, 'MAC': MAC}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce26447-f5df-4efd-87c6-563209dc72dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bfad6-6ace-47fa-b6a4-a57dadf162e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(exp_config['save_file'], 'rb') as handle:\n",
    "    res_dict = pickle.load(handle)\n",
    "    \n",
    "res_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2cd77e-eaa7-402d-9bb1-27aab6679042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = res_dict['params']\n",
    "results = res_dict['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07338340-8162-4555-906b-bea8654b558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist_models = ['google/electra-small-generator', \"albert-xlarge-v2\", \"albert-xxlarge-v2\", \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\", \"nlpaueb/legal-bert-base-uncased\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc79ef1-51c1-437b-a714-05d7603c466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = list(cosine_scores.keys())+['extrinsic']\n",
    "scores_per_bias_type = {bt : {score: [] for score in score_list} for bt in exp_config['bias_types']}\n",
    "sample_corr = {bt : {score: [] for score in score_list[:-1]} for bt in exp_config['bias_types']}\n",
    "for i in range(len(results)):\n",
    "    if params[i]['mlm'] in blacklist_models:\n",
    "        continue\n",
    "    if 'mlm' not in params[i].keys():\n",
    "        continue\n",
    "    #print(\"experiment\", i, \"with bias type\", params[i]['bias_type'])\n",
    "    for score in score_list:\n",
    "        scores_per_bias_type[params[i]['bias_type']][score].append(results[i][score])\n",
    "    \n",
    "    print(results[i].keys())\n",
    "    for score in score_list:\n",
    "        if not score+'_individual' in results[i].keys():\n",
    "            continue\n",
    "        for score2 in score_list:\n",
    "            if score == score2:\n",
    "                continue # TODO \n",
    "            if not score2+'_individual' in results[i].keys():\n",
    "                continue\n",
    "            if score == 'extrinsic':\n",
    "                # extrinsic vs. cosine score\n",
    "                #slope, intercept, r, p, std_err = scipy.stats.linregress(np.abs(results[i][score+'_individual']), np.abs(results[i][score2+'_individual'])) # this doesnt work\n",
    "                slope, intercept, r, p, std_err = scipy.stats.linregress(np.abs(results[i][score+'_individual']), np.abs(results[i][score2+'_cs']))\n",
    "                \n",
    "                \n",
    "                if p < 0.05:\n",
    "                    print(score, score2, \"R=\"+str(r)+\" (p=\"+str(p)+\")\")\n",
    "                    sample_corr[params[i]['bias_type']][score2].append(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd451c9-65c5-423a-aed4-be15c89d11a3",
   "metadata": {},
   "source": [
    "## Sample bias correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ae18f-c459-44bb-b304-965d292a67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aaf79d-f043-4e8f-b34d-444026c9d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_axis_style(ax, labels):\n",
    "    ax.set_xticks(np.arange(1, len(labels) + 1), labels=labels)\n",
    "    ax.set_xlim(0.25, len(labels) + 0.75)\n",
    "    #ax.set_xlabel('Sample name')\n",
    "    ax.hlines(0,0.25, len(labels) + 0.75, 'grey', '--', linewidths=0.8)\n",
    "\n",
    "for bt, res in sample_corr.items():    \n",
    "    data = []\n",
    "    labels = []\n",
    "    for score in score_list[:-1]:\n",
    "        if len(res[score]) > 0: # skip gWEAT/ WEAT for non-binary\n",
    "            data.append(res[score])\n",
    "            labels.append(score)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 9), sharey=True)\n",
    "    \n",
    "    # set style for the axes\n",
    "    set_axis_style(ax, labels)\n",
    "\n",
    "    ax.set_title(exp_config['bias_types'][0])\n",
    " #   ax.set_title('Pearson Correlation with PLL(more)-PLL(less)')# of class-wise cosine scores with the TP GAP')\n",
    "    ax.set_ylabel('Pearson Coefficient R')\n",
    "    ax.violinplot(data)\n",
    "    \n",
    "    plt.subplots_adjust(bottom=0.15, wspace=0.05)\n",
    "    plt.savefig('plots/mlm_sample_bias_'+bt+'.png', bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66de10d-0b1d-486e-91dc-0d156b8376ed",
   "metadata": {},
   "source": [
    "## Aggregated bias correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74802ed6-4676-4edc-bb7b-0e882ef8c3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for bt, res in scores_per_bias_type.items():\n",
    "    df = pd.DataFrame(res)\n",
    "    print(bt)\n",
    "    for score in score_list:\n",
    "        if not score == 'extrinsic':\n",
    "            print(score)\n",
    "            slope, intercept, r, p, std_err = scipy.stats.linregress(res[score], res[score2])\n",
    "            print(\"R=\"+str(r)+\" (p=\"+str(p)+\")\")\n",
    "            print()\n",
    "            \n",
    "            ax = sns.regplot(x=\"extrinsic\", y=score, data=df).set_title(exp_config['bias_types'][0]+\" R=%.3f (p=%.4f)\" % (r,p))\n",
    "            #ax.set(xlabel='% PLL(more) > PLL(less)')\n",
    "            plt.savefig('plots/mlm_aggr_corr_'+score+'_'+bt+'.png', bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df92dac-a423-4a98-ae05-ef916a17e85d",
   "metadata": {},
   "source": [
    "## Can we distinguish more/less biased models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8bbc2b-25e0-40d2-af14-1aa048c2aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_low_bias = []\n",
    "exp_high_bias = []\n",
    "#bias_type = 'religion'\n",
    "\n",
    "biases = [np.mean(results[i]['extrinsic']) for i in range(len(results))]# if params[i]['bias_type'] == bias_type]\n",
    "valid_exp_ids = [i for i in range(len(results))]# if params[i]['bias_type'] == bias_type]\n",
    "counts, bins = np.histogram(biases)\n",
    "plt.hist(bins[:-1], bins, weights=counts)\n",
    "plt.show()\n",
    "\n",
    "mean = np.mean(biases)\n",
    "std = np.std(biases)\n",
    "for i in range(len(biases)):\n",
    "    if biases[i] < mean-std:\n",
    "        exp_low_bias.append(valid_exp_ids[i])\n",
    "    if biases[i] > mean+std:\n",
    "        exp_high_bias.append(valid_exp_ids[i])\n",
    "\n",
    "print(exp_low_bias)\n",
    "print(exp_high_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d444cd5-2ced-443c-9945-8af74020689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "bias_type = exp_config['bias_types'][0]\n",
    "def get_score_pred(exp_i, exp_j, score_key):\n",
    "    return int(results[exp_i][score_key] < results[exp_j][score_key])\n",
    "    \n",
    "extrinsic_pred = []\n",
    "for i in exp_low_bias:\n",
    "    for j in exp_high_bias:\n",
    "        extrinsic_pred.append(get_score_pred(i,j,'extrinsic'))\n",
    "for i in exp_high_bias:\n",
    "    for j in exp_low_bias:\n",
    "        extrinsic_pred.append(get_score_pred(i,j,'extrinsic'))\n",
    "print(len(extrinsic_pred))\n",
    "print(extrinsic_pred)\n",
    "\n",
    "for score in cosine_scores.keys():\n",
    "    \n",
    "    if bias_type in ['age', 'gender'] and score == 'gWEAT': # binary experiment\n",
    "        continue\n",
    "    if bias_type in ['race-color', 'religion'] and score == 'WEAT':\n",
    "        continue\n",
    "    print(score)\n",
    "    \n",
    "    score_pred = []\n",
    "    for i in exp_low_bias:\n",
    "        for j in exp_high_bias:\n",
    "            score_pred.append(get_score_pred(i,j,score))\n",
    "    for i in exp_high_bias:\n",
    "        for j in exp_low_bias:\n",
    "            score_pred.append(get_score_pred(i,j,score))\n",
    "    print(score_pred)\n",
    "\n",
    "    cm = confusion_matrix(extrinsic_pred, score_pred, normalize='true')\n",
    "    cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "    cm_display.ax_.get_images()[0].set_clim(0, 1)\n",
    "    cm_display.ax_.get_images()[0].set_cmap(plt.cm.Blues)\n",
    "    cm_display.ax_.set_title(score)\n",
    "    plt.savefig('plots/cm_bias_pred_'+score+'.png', bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"ROC AUC: \", roc_auc_score(extrinsic_pred, score_pred))\n",
    "    print(\"accuracy: \", accuracy_score(extrinsic_pred, score_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840811ea-0c2a-424a-8d10-e5f5376972d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
